{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup\n",
    "\n",
    "Unless you have a powerful GPU, running feature extraction on these models will take a significant amount of time. To make things easier we precomputed bottleneck features for each network. This will allow you experiment with feature extraction even on a modest CPU. You can think of bottleneck features as feature extraction but with caching. Because the base network weights are frozen during feature extraction, the output for an image will always be the same. Thus, once the image has already been passed through the network, we can cache and reuse the output.\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    {network}_features.npz\n",
    "\n",
    "where `network`, in the above filename, can be one of `vgg16`, `vgg19`, `inception`, `xception`, or `resnet50`.\n",
    "\n",
    "# Extraction\n",
    "\n",
    "Then, to obtain the bottleneck features corresponding to the train, test, and validation sets, you should run:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/{network}_features.npz')\n",
    "    train_{network} = bottleneck_features['train_{network}']\n",
    "    valid_{network} = bottleneck_features['valid_{network}']\n",
    "    test_{network} = bottleneck_features['test_{network}']\n",
    "    \n",
    "where `network`, in the above filename, can be one of `vgg16`, `vgg19`, `inception`, `xception`, or `resnet50`.\n",
    "\n",
    "# Example Code\n",
    "\n",
    "To obtain the vgg-16 bottleneck features corresponding to the train, test, and validation sets, you should run:\n",
    "    \n",
    "    bottleneck_features = np.load('bottleneck_features/vgg16_features.npz')\n",
    "    train_vgg16 = bottleneck_features['train_vgg16']\n",
    "    valid_vgg16 = bottleneck_features['valid_vgg16']\n",
    "    test_vgg16 = bottleneck_features['test_vgg16']\n",
    "\n",
    "# Appendix\n",
    "\n",
    "If you'd like to see the exact code we used to compute the bottleneck features, please look below.  Each block of code is annotated with the size of the extracted bottleneck features.\n",
    "    \n",
    "### Xception\n",
    "\n",
    "```\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "base_model = Xception(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.layers[-1].output)\n",
    "# (nb_batches, 7, 7, 2048)\n",
    "train_xception = model.predict(preprocess_input(train_tensors))\n",
    "valid_xception = model.predict(preprocess_input(valid_tensors))\n",
    "test_xception = model.predict(preprocess_input(test_tensors))\n",
    "\n",
    "np.savez('xception_features', train_xception=train_xception, \n",
    "         valid_xception=valid_xception, test_xception=test_xception)\n",
    "```\n",
    "\n",
    "### Inception\n",
    "\n",
    "```\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.layers[-1].output)\n",
    "# (nb_batches, 5, 5, 2048)\n",
    "train_inception = model.predict(preprocess_input(train_tensors))\n",
    "valid_inception = model.predict(preprocess_input(valid_tensors))\n",
    "test_inception = model.predict(preprocess_input(test_tensors))\n",
    "\n",
    "np.savez('inception_features', train_inception=train_inception, \n",
    "         valid_inception=valid_inception, test_inception=test_inception)\n",
    "```\n",
    "\n",
    "### ResNet50\n",
    "\n",
    "```\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "                        \n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.layers[-1].output)\n",
    "# (nb_batches, 1, 1, 2048)\n",
    "train_resnet50 = model.predict(preprocess_input(train_tensors))\n",
    "valid_resnet50 = model.predict(preprocess_input(valid_tensors))\n",
    "test_resnet50 = model.predict(preprocess_input(test_tensors))\n",
    "\n",
    "np.savez('resnet50_features', train_resnet50=train_resnet50, \n",
    "         valid_resnet50=valid_resnet50, test_resnet50=test_resnet50)\n",
    "```\n",
    "\n",
    "### VGG-19\n",
    "\n",
    "```\n",
    "from keras.applications.vgg19 import VGG19\n",
    "                        \n",
    "base_model = VGG19(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.layers[-1].output)\n",
    "# (nb_batches, 7, 7, 512)\n",
    "train_vgg19 = model.predict(preprocess_input(train_tensors))\n",
    "valid_vgg19 = model.predict(preprocess_input(valid_tensors))\n",
    "test_vgg19 = model.predict(preprocess_input(test_tensors))\n",
    "\n",
    "np.savez('vgg19_features', train_vgg19=train_vgg19, \n",
    "         valid_vgg19=valid_vgg19, test_vgg19=test_vgg19)\n",
    "```\n",
    "\n",
    "### VGG-16\n",
    "\n",
    "```\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.layers[-1].output)\n",
    "# (nb_batches, 7, 7, 512)\n",
    "train_vgg16 = model.predict(preprocess_input(train_tensors))\n",
    "valid_vgg16 = model.predict(preprocess_input(valid_tensors))\n",
    "test_vgg16 = model.predict(preprocess_input(test_tensors))\n",
    "\n",
    "np.savez('vgg16_features', train_vgg16=train_vgg16, valid_vgg16=valid_vgg16, test_vgg16=test_vgg16)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load ordered list of dog names\n",
    "dog_names = [item[25:-1] for item in glob(\"dogImages/train/*/\")]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % str(len(train_files) + len(valid_files) + len(test_files)))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image   \n",
    "from PIL import ImageFile                          \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "# get tensors suitable for supplying to keras\n",
    "train_tensors = paths_to_tensor(train_files)\n",
    "valid_tensors = paths_to_tensor(valid_files)\n",
    "test_tensors = paths_to_tensor(test_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
